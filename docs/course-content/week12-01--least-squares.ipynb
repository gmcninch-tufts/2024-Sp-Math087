{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [George McNinch](http://gmcninch.math.tufts.edu) Math 87 - Spring 2024\n",
    "\n",
    "# Week 12\n",
    "\n",
    "# Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Linear Least Squares\n",
    "====================\n",
    "\n",
    "We are going to begin our discussion of \"least squares\" approximation with an example.\n",
    "\n",
    "Example\n",
    "-------\n",
    "\n",
    "Consider a stretch of highway with four distinct reference points **A**, **B**, **C**, and **D**:\n",
    "\n",
    "[**A**] ----``x1``----- [**B**] ---``x2``------ [**C**] ---``x3``------[**D**]  \n",
    "\n",
    "Write ``x1`` = **AB** for the distance from **A** to **B**, ``x2`` = **BC**, ``x3`` = **CD**.\n",
    "\n",
    "We take some measurements -- which potentially reflect errors -- , and we seek the *best approximation* to the distances ``x1, x2, x3``.\n",
    "\n",
    "The measurements taken are as follows:\n",
    "\n",
    "|||||||\n",
    "| ------: | ------:| ------:| ------:| ------:| ------:|\n",
    "| segment | **AD** | **AC** | **BD** | **AB** | **CD** |\n",
    "| length  | 89 m   | 67 m   | 53 m   | 35 m   | 20 m   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus the observations suggest the following equations:\n",
    "\n",
    "``(1)  x1 + x2 + x3 = 89``  \n",
    "``(2)  x1 + x2      = 67``  \n",
    "``(3)       x2 + x3 = 53``  \n",
    "``(4)  x1           = 35``  \n",
    "``(5)            x3 = 20``\n",
    "\n",
    "These equations aren't compatible, though. Note e.g. that equations ``(3) -- (5)`` indicate the following:\n",
    "\n",
    "``x1 = 35, x3 = 20, x2 = 53-20 = 33``\n",
    "\n",
    "but then we find that\n",
    "\n",
    "``x1 + x2 + x3 = 35 + 33 + 20 = 88``\n",
    "\n",
    "which is incompatible with ``(1)``.\n",
    "\n",
    "And we find that \n",
    "\n",
    "``x1 + x2 = 35 + 33 = 68`` \n",
    "\n",
    "which is incompatible with ``(2)``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Let's formulate these equalities in matrix form.\n",
    "\n",
    "Thus let \n",
    "$$A = \\begin{pmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{pmatrix}, \\quad\n",
    "\\mathbf{x} = \\begin{pmatrix}\n",
    "x_1 \\\\ x_2 \\\\ x_3 \n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\mathbf{b} = \\begin{pmatrix}\n",
    "89 \\\\ 67 \\\\ 53 \\\\ 35 \\\\ 20\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "With these notations, the above equations suggest that $A\\mathbf{x}$ should be equal to $\\mathbf{b}$.\n",
    "\n",
    "Our observation(s) in the preceding slides show, however, that the system of equations\n",
    "$A \\mathbf{x} = \\mathbf{b}$\n",
    "is *inconsistent* (i.e. there is no vector $\\mathbf{x}$ which makes the equation true).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Residual\n",
    "--------\n",
    "\n",
    "In general, given an $m\\times n$ matrix $A$, a column vector $\\mathbf{b} \\in \\mathbb{R}^m$ and an equation\n",
    "$A \\mathbf{x} = \\mathbf{b}$, we instead look at the so-called *residual*\n",
    "\n",
    "$$\\mathbf{r} = \\mathbf{b} - \\mathbf{A}\\mathbf{x},$$\n",
    "\n",
    "and *minimize* this residual.\n",
    "\n",
    "More precisely, we want to minimize the *magnitude* (or *length*) of this vector.\n",
    "\n",
    "Thus if $\\mathbf{r} = \\begin{pmatrix} r_1 & \\cdots & r_m \\end{pmatrix}^T$, we must minimize the quantity\n",
    "\n",
    "$$\\Vert\\mathbf{r}\\Vert = \\left(\\sum_{i=1}^m r_i^2 \\right)^{1/2}$$\n",
    "\n",
    "Here $\\Vert \\mathbf{r} \\Vert$ is the magnitude, also called the Euclidean norm, of the vector $\\mathbf{r}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "In fact, because $f(x) = \\sqrt{x}$ is an increasing function of $x$, we instead minimize the *square* of the magnitude of $\\mathbf{r}$::\n",
    "\n",
    "$$\\Vert\\mathbf{r}\\Vert^2 = \\sum_{i=1}^m r_i^2$$\n",
    "\n",
    "Thus, we wish to find\n",
    "\n",
    "$$\\min_{\\mathbf{x}} \\Vert \\mathbf{r} \\Vert^2 = \\min_{\\mathbf{x}} \\Vert \\mathbf{b} - A \\mathbf{x} \\Vert^2\n",
    "= \\min_{x_1,x_2,\\dots,x_n} \\sum_{i=1}^m \\left( b_i - \\sum_{j=1}^n A_{ij} x_j \\right)^2$$\n",
    "\n",
    "The idea behind this minimization is to first compute for $1 \\le k \\le n$ \n",
    "the partial derivatives $\\dfrac{\\partial F}{\\partial x_k}$ of the function\n",
    "$$F(x_1,x_2,\\dots,x_n)  = \\sum_{i=1}^m \\left( b_i - \\sum_{j=1}^n A_{ij} x_j \\right)^2$$\n",
    "\n",
    "Critical points - and thus possible minima - for $F$ occur at points $\\mathbf{x}$ for which\n",
    "all $\\dfrac{\\partial F}{\\partial x_k}(\\mathbf{x}) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Now,\n",
    "$$\\dfrac{\\partial F}{\\partial x_k} = \\sum_{i=1}^m 2\\left(b_i - \\sum_{j=1}^n A_{ij} x_j\\right)(-A_{ik})\n",
    "= 2\\left( \\sum_{i=1}^m (-A_{ik}b_i) + \\sum_{i=1}^m A_{ik} \\sum_{j=1}^n A_{ij}x_j \\right)$$\n",
    "\n",
    "and this expression is equal to the $k$-th coefficient of the vector\n",
    "$$2\\left(-A^T \\mathbf{b} + A^T A \\mathbf{x} \\right)$$\n",
    "\n",
    "Thus, the condition $\\dfrac{\\partial F}{\\partial x_k} = 0$ for all $k$ is equivalent to the so-called *normal equations*:\n",
    "\n",
    "$$(\\diamondsuit) \\quad A^T A \\mathbf{x} = A^T \\mathbf{b}.$$\n",
    "\n",
    "Thus the solutions $\\mathbf{x}$ to the normal equations $(\\diamondsuit)$ are precisely the critical points of the function $F$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Recall that $A \\in \\mathbb{R}^{m \\times n}$. Thus, $A^T \\in \\mathbb{R}^{n \\times m}$ so that \n",
    "the matrix $A^TA$ is $n \\times n$; in particular, $A^T A$ is always a square matrix.\n",
    "\n",
    "Moreover, $A^T A$ is *symmetric*, since\n",
    "\n",
    "$$(A^T A)^T = A^T (A^T)^T = A^T A.$$\n",
    "\n",
    "We are interested here in the case of *overdetermined systems* -- i.e. in the case where $A$ has more rows than columns (\"more equations than variables\"). Thus $m \\ge n$.\n",
    "\n",
    "We also are interested in the case where $A$ has rank $n$ -- i.e.$A$ has $n$ linearly independent columns -- since otherwise we don't expect to have enough information to find $\\mathbf{x}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Proposition\n",
    "-----------\n",
    "\n",
    "Let $A \\in \\mathbb{R}^{m \\times n}$, suppose that $m \\ge n$ and that $A$ has rank $n$.\n",
    "Then $A^T \\cdot A$ is invertible.\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "Since $A^T \\cdot A$ is an $n \\times n$ square matrix, the proposition will follow if we argue\n",
    "that the null space $\\operatorname{Null}(A^TA)$ is zero. So: suppose that $\\mathbf{v} \\in \\operatorname{Null}(A^TA)$.\n",
    "\n",
    "Thus $A^TA\\mathbf{v} = 0$ and thus also $\\mathbf{v}^T A^T \\cdot A \\mathbf{v} = 0$.\n",
    "\n",
    "Now,\n",
    "$$0 = \\mathbf{v}^T A^T \\cdot A \\mathbf{v} = (A \\mathbf{v})^T(A \\mathbf{v})$$\n",
    "\n",
    "and of course for any vector $\\mathbf{w}$, we know that \n",
    "$$0 = \\mathbf{w}^T \\mathbf{w} \\implies \\mathbf{w} = \\mathbf{0}.$$\n",
    "\n",
    "We now conclude that $A\\mathbf{v} = 0$, so $\\mathbf{v} \\in \\operatorname{Null}(A)$. Since\n",
    "$A$ has rank $n$, the Null space of $A$ is equal to zero, and we condlue that $\\mathbf{v} = \\mathbf{0}$. \n",
    "\n",
    "We have now proved that $\\operatorname{Null}(A^TA)$ is zero, as required.\n",
    "\n",
    "**Remark:** What we have really showed is that the symmetric matrix $A^T A$ is *definite*:\n",
    "$\\mathbf{v}^TA^T A \\mathbf{v} = 0 \\implies \\mathbf{v} = \\mathbf{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "We finally claim that this solution must minimize the magnitude of the residual.\n",
    "\n",
    "This depends on a \"second derivative test\" argument for which I'm not going to give full details.\n",
    "The main point is that the \"second derivative\" in this context -- known as the Hessian -- concides\n",
    "with the matrix $2A^T A$. Now, under our assumptions the matrix $A^T A$ is postive definite, and it follows\n",
    "that $\\mathbf{x}_0$ is a global minimum for the magnitude of the residual!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Return to the example\n",
    "----------------------\n",
    "\n",
    "Recall that\n",
    "$$A = \\begin{pmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{pmatrix}, \\quad\n",
    "\\mathbf{x} = \\begin{pmatrix}\n",
    "x_1 \\\\ x_2 \\\\ x_3 \n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\mathbf{b} = \\begin{pmatrix}\n",
    "89 \\\\ 67 \\\\ 53 \\\\ 35 \\\\ 20\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "So to minimize the magnitude of the residual, we must solve the normal equations:\n",
    "\n",
    "$$A^T A \\mathbf{x} = A^T \\mathbf{b}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Now \n",
    "$$A^T A = \n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 0 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 0 & 0 \\\\\n",
    "1 & 0 & 1 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 & 1 & 1 \\\\\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 0 & 1 \n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "3 & 2 & 1 \\\\\n",
    "2 & 3 & 2 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "and\n",
    "$$A^T \\mathbf{b} = \\begin{pmatrix}\n",
    "191 \\\\ 209\\\\ 162\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "So we need to solve the equation\n",
    "\n",
    "$$\\begin{pmatrix}\n",
    "3 & 2 & 1 \\\\\n",
    "2 & 3 & 2 \\\\\n",
    "1 & 2 & 3\n",
    "\\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "191 \\\\ 209\\\\ 162\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([35.125, 32.5  , 20.625]), 1.1726039399558574]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "\n",
    "A= np.array([[1,1,1],[1,1,0],[0,1,1],[1,0,0],[0,0,1]])\n",
    "b = np.array([89,67,53,35,20])\n",
    "\n",
    "\n",
    "x0=la.solve(A.T @ A, A.T @ b)\n",
    "\n",
    "def residual(x):\n",
    "    return b - A @ x\n",
    "\n",
    "def magnitude(x):\n",
    "    return np.sqrt(x@x)\n",
    "    \n",
    "[x0,magnitude(residual(x0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus the *least squares solution* is\n",
    "\n",
    "$$\\mathbf{x}_0 = \\begin{pmatrix}\n",
    "35.125 \\\\ 32.5 \\\\ 20.625\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\Vert \\mathbf{b} - A \\mathbf{x}_0\\Vert \\approx 1.1726$$\n",
    "\n",
    "Recall that our \"first guess\" for a solution (based on some of the measurements) was\n",
    "\n",
    "$$\\mathbf{x}_1 = \\begin{pmatrix}\n",
    "35 \\\\ 33 \\\\ 20\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The residual is indeed larger for $x_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([35,33,20])\n",
    "\n",
    "magnitude(residual(x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's note that ``numpy`` already implements this least-squares functionality:\n",
    "\n",
    "-- you can [read more about it here](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html#numpy.linalg.lstsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35.125, 32.5  , 20.625])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=la.lstsq(A,b,rcond=None)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example recapitulated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a group of 10 employees `[a,b,c,...,h,i,j]` of a certain company. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
